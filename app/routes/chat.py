from flask import Blueprint, request, jsonify, render_template
import ollama
import logging
import time
from datetime import datetime
import os
import psutil
try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
import traceback  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ —Å—Ç–µ–∫—Ç—Ä–µ–π—Å–∞
import logging.handlers  # –î–æ–±–∞–≤–ª—è–µ–º handlers
from functools import lru_cache

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
log_dir = 'logs'
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–æ–≤
log_file = os.path.join(log_dir, 'app.log')

# –°–æ–∑–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä –¥–ª—è –ª–æ–≥–æ–≤
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setFormatter(formatter)

# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π –ª–æ–≥–≥–µ—Ä
logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
for handler in logger.handlers[:]:
    logger.removeHandler(handler)

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à–∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏ –∑–∞–ø–∏—Å—å
logger.info('=== –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ===')
logger.info(f'–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ. –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–æ–≤: {log_file}')

try:
    with open(log_file, 'a') as f:
        f.write('–¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–ø–∏—Å—å –≤ –ª–æ–≥\n')
    logger.info('–¢–µ—Å—Ç –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª —É—Å–ø–µ—à–µ–Ω')
except Exception as e:
    logger.error(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª: {e}')
    print(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª: {e}')

chat_bp = Blueprint('chat', __name__)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ
SYSTEM_PROMPT = "–¢—ã Eva - –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ."

def get_system_stats():
    stats = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'cpu': {
            'percent': psutil.cpu_percent(interval=1),
            'freq_mhz': psutil.cpu_freq().current if psutil.cpu_freq() else 'N/A',
            'cores_total': psutil.cpu_count(),
            'cores_physical': psutil.cpu_count(logical=False)
        },
        'memory': {
            'total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
            'used_gb': round(psutil.virtual_memory().used / (1024**3), 2),
            'percent': psutil.virtual_memory().percent
        },
        'process': {
            'cpu_percent': psutil.Process().cpu_percent(),
            'memory_percent': psutil.Process().memory_percent(),
            'threads': len(psutil.Process().threads())
        }
    }
    
    # GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if GPU_AVAILABLE:
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                stats['gpu'] = {
                    'name': gpu.name,
                    'load_percent': round(gpu.load * 100, 2),
                    'memory_total_mb': gpu.memoryTotal,
                    'memory_used_mb': gpu.memoryUsed,
                    'temperature_c': gpu.temperature
                }
        except Exception as e:
            stats['gpu'] = f'Error getting GPU stats: {str(e)}'
    
    return stats

def format_stats(stats):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—ÉÔøΩÔøΩ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –ª–æ–≥"""
    return f"""=== –°–∏—Å—Ç–µ–º–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ===
CPU:
    –ó–∞–≥—Ä—É–∑–∫–∞: {stats['cpu']['percent']}%
    –ß–∞—Å—Ç–æ—Ç–∞: {stats['cpu']['freq_mhz']} MHz
    –Ø–¥—Ä–∞: {stats['cpu']['cores_physical']} —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö / {stats['cpu']['cores_total']} –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö

–ü–∞–º—è—Ç—å:
    –í—Å–µ–≥–æ: {stats['memory']['total_gb']:.1f} GB
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {stats['memory']['used_gb']:.1f} GB ({stats['memory']['percent']}%)

–ü—Ä–æ—Ü–µ—Å—Å:
    CPU: {stats['process']['cpu_percent']}%
    RAM: {stats['process']['memory_percent']:.1f}%
    –ü–æ—Ç–æ–∫–∏: {stats['process']['threads']}

GPU: {stats['gpu'] if 'gpu' in stats else '–ù–µ –¥–æ—Å—Ç—É–ø–µ–Ω'}"""

# –ö—ç—à –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
@lru_cache(maxsize=100)
def get_cached_response(message, temperature):
    """–ö—ç—à–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –¥–ª—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    return ollama.chat(
        model='mistral:7b',
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": message}
        ],
        stream=False,
        options={
            "temperature": temperature,
            "top_p": 0.9,
            "max_tokens": 100,
            "num_ctx": 256,
            "num_gpu": 1,
            "num_thread": 16,
            "repeat_penalty": 1.1,
            "stop": ["Human:", "Assistant:"],
            "num_batch": 2048,
            "f16_kv": True,
            "rope_frequency_base": 10000,
            "rope_frequency_scale": 0.5,
            "low_vram": True,
            "cache_mode": "always",
            "seed": 42,
            "num_predict": 100
        }
    )

@chat_bp.route('/')
def chat_page():
    return render_template('ai_assistant.html')

@chat_bp.route('/message', methods=['POST'])
def chat():
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥—è—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        data = request.json
        if not data:
            logger.error("–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å")
            return jsonify({"error": "–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"}), 400

        user_message = data.get('message', '')
        if not user_message:
            logger.error("–°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return jsonify({"error": "–°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"}), 400

        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {user_message}")
        start_time = time.time()

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        try:
            start_stats = get_system_stats()
            stats_text = format_stats(start_stats)
            logger.info("–ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
            logger.info(stats_text)
        except Exception as stats_error:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {stats_error}")
            logger.error(traceback.format_exc())
            start_stats = {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"}

        # –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
        if user_message.lower() in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', 'hi', 'hello']:
            greeting = "–ü—Ä–∏–≤–µ—Ç! –Ø Eva - —Ç–≤–æ–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫! üåü –†–∞–¥–∞ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å!"
            return jsonify({"response": greeting})

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è —á–∞—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            response = get_cached_response(user_message, temperature=0.7)
            ai_message = response['message']['content'].strip()
            logger.info(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏: {ai_message}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ø–∞–¥–∞–Ω–∏–µ –≤ –∫—ç—à
            if hasattr(get_cached_response, 'cache_info'):
                logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞: {get_cached_response.cache_info()}")

        except Exception as model_error:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {model_error}")
            logger.error(traceback.format_exc())
            return jsonify({"error": "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"}), 500

        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        generation_time = time.time() - start_time
        try:
            end_stats = get_system_stats()
            stats_text = format_stats(end_stats)
            logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {generation_time:.2f} —Å–µ–∫")
            logger.info(stats_text)
        except Exception as stats_error:
            logger.error(f"–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {stats_error}")
            logger.error(traceback.format_exc())
            end_stats = {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"}

        return jsonify({
            "response": ai_message,
            "stats": {
                "generation_time": generation_time,
                "system_stats": end_stats
            }
        })

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "error": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞", 
            "details": str(e)
        }), 500

@chat_bp.route('/reset', methods=['POST'])
def reset_chat():
    logger.info("–°–±—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞")
    global chat_history
    chat_history = []
    return jsonify({"status": "Chat reset successfully"})
